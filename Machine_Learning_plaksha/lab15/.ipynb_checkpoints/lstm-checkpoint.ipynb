{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, LSTM, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaing the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'UCI HAR Dataset/')\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'UCI HAR Dataset/')\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model with Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit and evaluate an LSTM model (do 10-fold cross validation)\n",
    "# def evaluate_model(trainX, trainy, testX, testy):\n",
    "#     # define model\n",
    "#     verbose, epochs, batch_size = 0, 15, 64\n",
    "#     n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "#     print(n_timesteps, n_features, n_outputs)\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(n_outputs, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     # fit network\n",
    "#     model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "#     # evaluate model\n",
    "#     _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "#     return accuracy\n",
    "\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    print(n_timesteps, n_features, n_outputs)\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_model(trainX, trainy, testX, testy, n_neurons):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 15, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=n_neurons, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing the Results Obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores \n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=10, n_neurons=100):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_lstm_model(trainX, trainy, testX, testy, n_neurons)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Different Number of Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(repeats=5, n_neurons=100):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_lstm_model(trainX, trainy, testX, testy, n_neurons)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(repeats=5, n_neurons=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_lstm_model(trainX, trainy, testX, testy, n_neurons)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(repeats=5, n_neurons=200):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_lstm_model(trainX, trainy, testX, testy, n_neurons)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjective Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Why was this problem suitable for LSTM and nor just any other machine learning algorithm?**\n",
    "- This problem is suitable for LSTM (Long Short-Term Memory) and not just any other machine learning algorithm due to the temporal nature of the data. The data involves sequences of activities over time, and LSTM is particularly well-suited for handling sequences and capturing long-term dependencies. In this case, the LSTM can learn and remember patterns in the sequence of movement data, which is crucial for recognizing different activities that unfold over time. Traditional machine learning algorithms are not as effective in capturing temporal aspects of the data.\n",
    "\n",
    "\n",
    "**2. Is a many-to-many model a real-world scenario for this problem or a many-to-one model?**\n",
    "- A many-to-one model is more appropriate. The input sequences (sensor data over time) are mapped to a single output (activity label). The goal is to classify the entire sequence of movement data into one of the six activities. A many-to-many model might be more suitable if the task involved predicting a sequence of labels for each time step, which is not the case here.\n",
    "\n",
    "**3**. In LSTMs, \"Short Memory\" refers to remembering information over short sequences, while \"Long Memory\" refers to remembering information over longer sequences.\n",
    "\n",
    "**4**. An LSTM node consists of the input gate (controls input flow), the forget gate (controls forgetting old information), and the output gate (controls output based on input and memory).\n",
    "\n",
    "**5**. LSTMs address the vanishing gradient problem in RNNs by using memory cells and gating mechanisms, enabling better long-term dependency modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
