{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "* You need to code in this jupyter notebook only.\n",
    "* Download this notebook and import in your jupyter lab.\n",
    "* You need to write a partial code for step 0 to step 8 mentioned with prefix ##\n",
    "* Fill the blanks where it is instructed in comments. \n",
    "* Leave other codes, structure as it is.\n",
    "* Follow all the instructions commented in a cells.\n",
    "* Upload this jupyter notebook after completion with your partial code.\n",
    "* Also upload the resulting image showing all the selected points and boundary line between them after LDA analysis.\n",
    "* Duetime: 1:30 PM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np ## import numpy\n",
    "import cv2 ## import opencv\n",
    "import matplotlib ## import matplotlib\n",
    "import matplotlib.pyplot as plt ## import matplotlib pyplot\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis ## from sklearn import LDA analysis\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "##---------------------------------------------------\n",
    "## Step 0: Install all other dependencies that occue at run time if  any module not found.\n",
    "##---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_points = 20 ## Number of points you want select from each strip. Recommended >= 20 \n",
    "\n",
    "img = cv2.imread(\"Indian_Flag.jpg\") ## Read the given image\n",
    "\n",
    "def select_points(img, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    #------------------------------------------\n",
    "    ## step 1: Convert the img from BGR to RGB using cv2 and display it using cv2.imshow\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #______________________________________\n",
    "    ## step 2: Put title of the image\n",
    "    plt.title(title)\n",
    "    ax.imshow(img)\n",
    "    ##_____________________________________\n",
    "    ##-----------------------------------------\n",
    "    \n",
    "    # Set the cursor style to a plus sign\n",
    "    fig.canvas.manager.set_window_title('Select Points')\n",
    "    cursor = matplotlib.widgets.Cursor(ax, useblit=True, color='red', linewidth=1)\n",
    "    plt.show(block=False)  # Show the image without blocking\n",
    "\n",
    "    k = 0\n",
    "    points = [] ## Create here an empty list to store points \n",
    "\n",
    "    while k < Number_of_points:\n",
    "        xy = plt.ginput(1, timeout=0)  # Non-blocking input\n",
    "        if len(xy) > 0:\n",
    "            col, row = map(int, xy[0])  # Convert to integer\n",
    "            ##-----------------------------------------------\n",
    "            ## Step 3: Collect RGB values at the clicked positions (col, row) and print it. \n",
    "            \n",
    "            print(\"RGB values at position (\", row, \",\", col, \") are: \", img[row, col])\n",
    "            ##-----------------------------------------------\n",
    "\n",
    "\n",
    "            k += 1\n",
    "            points.append([row, col, img[row, col]])  # Store RGB values in empty list points.\n",
    "            \n",
    "            # Display colored dot on the image\n",
    "            plt.scatter(col, row, c='black', marker='o', s=10)\n",
    "\n",
    "            # Redraw the image to include the dot\n",
    "            plt.draw()\n",
    "\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB values at position ( 275 , 637 ) are:  [246  79   9]\n",
      "RGB values at position ( 277 , 589 ) are:  [246  84  19]\n",
      "RGB values at position ( 275 , 555 ) are:  [243  89  19]\n",
      "RGB values at position ( 287 , 531 ) are:  [242  84  21]\n",
      "RGB values at position ( 288 , 553 ) are:  [245  91  21]\n",
      "RGB values at position ( 281 , 561 ) are:  [243  89  17]\n",
      "RGB values at position ( 298 , 574 ) are:  [240  84  10]\n",
      "RGB values at position ( 289 , 606 ) are:  [246  82  11]\n",
      "RGB values at position ( 277 , 603 ) are:  [245  84  16]\n",
      "RGB values at position ( 294 , 616 ) are:  [251  83  18]\n",
      "RGB values at position ( 273 , 649 ) are:  [242  75   4]\n",
      "RGB values at position ( 265 , 668 ) are:  [243  75   4]\n",
      "RGB values at position ( 286 , 659 ) are:  [238  71   1]\n",
      "RGB values at position ( 288 , 630 ) are:  [254  86  15]\n",
      "RGB values at position ( 278 , 619 ) are:  [244  81  14]\n",
      "RGB values at position ( 288 , 583 ) are:  [241  80  10]\n",
      "RGB values at position ( 271 , 572 ) are:  [250 101  19]\n",
      "RGB values at position ( 296 , 552 ) are:  [250  91  23]\n",
      "RGB values at position ( 276 , 541 ) are:  [241  86  19]\n",
      "RGB values at position ( 295 , 592 ) are:  [245  84  14]\n",
      "RGB values at position ( 308 , 525 ) are:  [202 201 209]\n",
      "RGB values at position ( 317 , 528 ) are:  [225 218 234]\n",
      "RGB values at position ( 320 , 540 ) are:  [224 223 228]\n",
      "RGB values at position ( 320 , 561 ) are:  [234 231 242]\n",
      "RGB values at position ( 329 , 545 ) are:  [221 220 234]\n",
      "RGB values at position ( 310 , 548 ) are:  [222 218 241]\n",
      "RGB values at position ( 306 , 539 ) are:  [230 210 219]\n",
      "RGB values at position ( 326 , 533 ) are:  [226 223 232]\n",
      "RGB values at position ( 331 , 523 ) are:  [211 213 226]\n",
      "RGB values at position ( 331 , 563 ) are:  [221 221 231]\n",
      "RGB values at position ( 314 , 572 ) are:  [239 228 236]\n",
      "RGB values at position ( 325 , 572 ) are:  [222 220 221]\n",
      "RGB values at position ( 335 , 574 ) are:  [228 226 239]\n",
      "RGB values at position ( 326 , 555 ) are:  [215 215 223]\n",
      "RGB values at position ( 312 , 555 ) are:  [228 217 231]\n",
      "RGB values at position ( 317 , 548 ) are:  [224 224 236]\n",
      "RGB values at position ( 312 , 626 ) are:  [223 225 220]\n",
      "RGB values at position ( 323 , 633 ) are:  [222 221 229]\n",
      "RGB values at position ( 322 , 637 ) are:  [228 225 242]\n",
      "RGB values at position ( 306 , 667 ) are:  [216 214 228]\n",
      "RGB values at position ( 343 , 517 ) are:  [ 44 102  78]\n",
      "RGB values at position ( 351 , 516 ) are:  [37 88 71]\n",
      "RGB values at position ( 358 , 526 ) are:  [24 99 69]\n",
      "RGB values at position ( 342 , 531 ) are:  [ 38 106  83]\n",
      "RGB values at position ( 368 , 529 ) are:  [ 32 101  73]\n",
      "RGB values at position ( 350 , 538 ) are:  [ 35 105  81]\n",
      "RGB values at position ( 365 , 543 ) are:  [29 99 71]\n",
      "RGB values at position ( 344 , 553 ) are:  [35 94 72]\n",
      "RGB values at position ( 371 , 553 ) are:  [30 94 68]\n",
      "RGB values at position ( 354 , 561 ) are:  [ 29 102  75]\n",
      "RGB values at position ( 363 , 573 ) are:  [25 93 68]\n",
      "RGB values at position ( 356 , 552 ) are:  [ 32 102  76]\n",
      "RGB values at position ( 371 , 571 ) are:  [33 96 67]\n",
      "RGB values at position ( 354 , 583 ) are:  [27 97 71]\n",
      "RGB values at position ( 371 , 585 ) are:  [31 98 67]\n",
      "RGB values at position ( 357 , 593 ) are:  [32 99 68]\n",
      "RGB values at position ( 367 , 595 ) are:  [18 87 56]\n",
      "RGB values at position ( 362 , 617 ) are:  [28 94 67]\n",
      "RGB values at position ( 354 , 607 ) are:  [31 96 72]\n",
      "RGB values at position ( 352 , 640 ) are:  [24 90 63]\n"
     ]
    }
   ],
   "source": [
    "##-----------------------------------------------------------------\n",
    "## Step4: fill the blanks for Selected points from saffron strip\n",
    "pts_saffron = select_points(img, \"saffron strip\")\n",
    "## Step5: fill the blanks for Selected points from white strip)\n",
    "pts_white = select_points(img, \"white strip\")\n",
    "## Step6: fill the blanks for Selected points from green strip\n",
    "pts_green = select_points(img, \"green strip\")\n",
    "##-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RGB values to Lab color space\n",
    "def rgb_to_lab(rgb):\n",
    "    return cv2.cvtColor(np.uint8([[rgb]]), cv2.COLOR_RGB2Lab)[0][0]\n",
    "\n",
    "saffron_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_saffron])\n",
    "white_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_white])\n",
    "green_lab = np.array([rgb_to_lab(rgb) for _, _, rgb in pts_green])\n",
    "\n",
    "## Step7: Extract a* and b* components from Lab color space\n",
    "a_features = np.hstack((saffron_lab[:, 1], white_lab[:, 1], green_lab[:, 1]))\n",
    "b_features = np.hstack((saffron_lab[:, 2], white_lab[:, 2], green_lab[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map class labels to numeric values\n",
    "class_mapping = {'Saffron': 0, 'White': 1, 'Green': 2}\n",
    "y = np.array([class_mapping[label] for label in ['Saffron'] * Number_of_points + ['White'] * Number_of_points + ['Green'] * Number_of_points])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(a_features[:Number_of_points], b_features[:Number_of_points], c='b', marker='o', s=50, label='Saffron')\n",
    "plt.scatter(a_features[Number_of_points:2*Number_of_points], b_features[Number_of_points:2*Number_of_points], c='g', marker='^', s=50, label='White')\n",
    "plt.scatter(a_features[2*Number_of_points:], b_features[2*Number_of_points:], c='r', marker='*', s=50, label='Green')\n",
    "plt.legend(['Saffron', 'White', 'Green'], loc='best')\n",
    "plt.xlabel('A component/feature') ## Provide x label\n",
    "plt.ylabel('B component/feature') ## Provide y label\n",
    "plt.title('Scatter Plot of Features A and B') ## Provide title\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "##------------------------------------------------------------\n",
    "# Step 8: Perform LDA analysis using LinearDiscriminantAnalysis() and lda.fit()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(np.column_stack((a_features, b_features)), y)\n",
    "\n",
    "\n",
    "##-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LDA boundaries\n",
    "plt.figure()\n",
    "plt.scatter(a_features[:Number_of_points], b_features[:Number_of_points], c='b', marker='o', s=50, label='Saffron')\n",
    "plt.scatter(a_features[Number_of_points:2*Number_of_points], b_features[Number_of_points:2*Number_of_points], c='g', marker='^', s=50, label='White')\n",
    "plt.scatter(a_features[2*Number_of_points:], b_features[2*Number_of_points:], c='r', marker='*', s=50, label='Green')\n",
    "\n",
    "plt.xlabel('Feature A')  ## Provide x label\n",
    "plt.ylabel('Feature B') ## Provide y label\n",
    "plt.title('LDA boundaries (linear model) for Colors of the Indian Flag')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx, yy, Z, colors='k', linewidths=2, linestyles='solid')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer following questions:\n",
    "1. What are the key assumptions underlying LDA, and how do these assumptions influence the model's performance?\n",
    "2. What are the hyperparameters in LDA, and how do they affect the outcome of the model?\n",
    "3. What methods can be employed to assess the effectiveness of an LDA model in terms of the separation of topics and the coherence of generated topics?\n",
    "4. What are some common challenges or limitations associated with LDA, and how can they be addressed or mitigated?\n",
    "5. What practical applications does this assignment have in real-world situations, and what benefits does it offer in those specific scenarios?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
